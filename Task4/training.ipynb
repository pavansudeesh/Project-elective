{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-18T08:26:49.885066Z","iopub.execute_input":"2021-10-18T08:26:49.885814Z","iopub.status.idle":"2021-10-18T08:26:50.047868Z","shell.execute_reply.started":"2021-10-18T08:26:49.885715Z","shell.execute_reply":"2021-10-18T08:26:50.047188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arr = os.listdir('../input/covid19-ct-scans/ct_scans')","metadata":{"execution":{"iopub.status.busy":"2021-10-18T08:26:50.049363Z","iopub.execute_input":"2021-10-18T08:26:50.049614Z","iopub.status.idle":"2021-10-18T08:26:50.054135Z","shell.execute_reply.started":"2021-10-18T08:26:50.049581Z","shell.execute_reply":"2021-10-18T08:26:50.053472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arr.sort()\nfor i in range(len(arr)):\n    print(arr[i])","metadata":{"execution":{"iopub.status.busy":"2021-10-18T08:26:50.055084Z","iopub.execute_input":"2021-10-18T08:26:50.05535Z","iopub.status.idle":"2021-10-18T08:26:50.066221Z","shell.execute_reply.started":"2021-10-18T08:26:50.055319Z","shell.execute_reply":"2021-10-18T08:26:50.065533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arr1 = os.listdir('../input/covid19-ct-scans/infection_mask')\narr1.sort()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T08:26:50.069389Z","iopub.execute_input":"2021-10-18T08:26:50.069573Z","iopub.status.idle":"2021-10-18T08:26:50.075372Z","shell.execute_reply.started":"2021-10-18T08:26:50.06955Z","shell.execute_reply":"2021-10-18T08:26:50.074657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfor i in range(len(arr1)):\n    print(arr1[i])","metadata":{"execution":{"iopub.status.busy":"2021-10-18T08:26:50.076585Z","iopub.execute_input":"2021-10-18T08:26:50.077361Z","iopub.status.idle":"2021-10-18T08:26:50.08798Z","shell.execute_reply.started":"2021-10-18T08:26:50.077335Z","shell.execute_reply":"2021-10-18T08:26:50.087129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nibabel as nib\nimport os\nimport torch\nimport torchvision.transforms as T\n\n# Data original source downloaded from : https://zenodo.org/record/3757476#.Xp0FhB9fgUE\n# Same data in Kaggle  : https://www.kaggle.com/andrewmvd/covid19-ct-scans\n\n# Data and mask path\ndata_path = \"../input/covid19-ct-scans/ct_scans/\"\nmask_path = \"../input/covid19-ct-scans/infection_mask/\"\n\n# Lung window in HU units\nHU_min = -1000\nHU_max = 400\n\n# names of files in data folder\n# arr = os.listdir(data_path)\n\n# select train and val subjects\ntrain_data_numbers = range(16, 19)  # data from coronacases.org\n# val_data_numbers = range(5, 6)  # data from radiopedia.org\n\n# Resizing function for image and mask according to the model input dimension\nresize_image = T.Resize(size=(572, 572))\n\n\n# function to preprocess data\ndef data_preprocess(path, hu_min, hu_max):\n    volume_data = nib.load(path)  # load data\n    volume_data_numpy = volume_data.get_fdata()  # get data as numpy\n    volume_data_tensor = torch.tensor(volume_data_numpy)  # convert to torch tensor\n    volume_data_tensor_clamped = torch.clamp(volume_data_tensor, min=hu_min, max=hu_max)  # apply HU lung window\n    volume_data_tensor_clamped_normalized = (volume_data_tensor_clamped-hu_min) / (hu_max-hu_min)  # normalize to [0,1]\n    return volume_data_tensor_clamped_normalized\n\n\n# function to obtain maask\ndef mask_obtain(fpath):\n    mask = nib.load(fpath)  # load mask\n    mask_numpy = mask.get_fdata()  # get mask as numpy\n    mask_tensor = torch.tensor(mask_numpy)  # convert to torch tensor\n    return mask_tensor\n\n\n# add zero padding if size of image less than required input size\ndef padding_size(slices):\n    if (572-slices.size()[1]) % 2 == 0:  # See if the difference between required size and data size is even or odd\n        # if difference is even, pad same number to either side\n        pad1 = (572-slices.size()[1]) // 2\n        pad2 = (572-slices.size()[1]) // 2\n    else:\n        # if difference is even, pad one side one value more than other\n        pad1 = (572-slices.size()[1]) // 2\n        pad2 = ((572-slices.size()[1]) // 2)+1\n\n    if (572-slices.size()[2]) % 2 == 0:  # See if the difference between required size and data size is even or odd\n        # if difference is even, pad same number to either side\n        pad3 = (572-slices.size()[2]) // 2\n        pad4 = (572-slices.size()[2]) // 2\n    else:\n        # if difference is even, pad one side one value more than other\n        pad3 = (572-slices.size()[2]) // 2\n        pad4 = ((572-slices.size()[2]) // 2)+1\n\n    return [pad4, pad3, pad2, pad1]  # return the number of zero padding in each side of the slice","metadata":{"execution":{"iopub.status.busy":"2021-10-18T08:26:50.088962Z","iopub.execute_input":"2021-10-18T08:26:50.089146Z","iopub.status.idle":"2021-10-18T08:26:54.789563Z","shell.execute_reply.started":"2021-10-18T08:26:50.089125Z","shell.execute_reply":"2021-10-18T08:26:54.788792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# for i in range(len(arr)) :\n#     print (arr[i])","metadata":{"execution":{"iopub.status.busy":"2021-10-18T08:26:54.792816Z","iopub.execute_input":"2021-10-18T08:26:54.79302Z","iopub.status.idle":"2021-10-18T08:26:54.798456Z","shell.execute_reply.started":"2021-10-18T08:26:54.792997Z","shell.execute_reply":"2021-10-18T08:26:54.797487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n# training data\n# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n# Initialize for stacking\ntrain_data = torch.empty((1, 572, 572))\ntrain_label = torch.empty((1, 572, 572))\n\n# Function to form train data and train label\nfor i in train_data_numbers:\n    file_path = data_path +arr[i]  # path of the data\n    data = data_preprocess(file_path, HU_min, HU_max)  # preprocess data\n    data = data.permute(2, 0,\n                        1)  # change the dimension (H,W,C) ---> (C,H,W) , since ConstantPad2d works with this config\n    P = padding_size(data)  # Obtain the required padding sizes\n    data = torch.nn.ConstantPad2d(P, 0)(data)  # pad the slices according to the padding sizes obtained\n    train_data = torch.cat((train_data, data), 0)  # stack the data(slices) along dimension C\n    file_path_mask = mask_path  +arr1[i]  # path to the mask\n    label = mask_obtain(file_path_mask)  # obtain the mask\n    # NOTE: Since we padded the data, mask should also have same size, so pad mask also\n    label = label.permute(2, 0,\n                          1)  # change the dimension (H,W,C) ---> (C,H,W) , since ConstantPad2d works with this config\n    label = torch.nn.ConstantPad2d(P, 0)(label)  # pad the maks according to the padding sizes of the slices\n    train_label = torch.cat((train_label, label), 0)  # stack the masks along dimension C\n\n# remove the empty\ntrain_data = train_data[1:train_data.size()[0], :, :]\ntrain_label = train_label[1:train_label.size()[0], :, :]\n# Determine which slices are all black\nidx = []\nfor i in range(train_label.size()[0]):\n    img_max = torch.max(train_label[i, :, :])\n    if img_max == 1:\n        idx.append(i)  # having white regions\n\n# Choose data without completely black mask, i.e, having atleast some white segmented region\ntrain_data_new = train_data[idx, :, :]\ntrain_label_new = train_label[idx, :, :]\n# (C,H,W) ---> (H,W,C) since Dataset class has this config (this part is not necessary if we change the config of\n# Dataset class)\ntrain_data_new = train_data_new.permute(1, 2, 0)\ntrain_label_new = train_label_new.permute(1, 2, 0)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T08:26:54.801498Z","iopub.execute_input":"2021-10-18T08:26:54.801769Z","iopub.status.idle":"2021-10-18T08:27:01.156952Z","shell.execute_reply.started":"2021-10-18T08:26:54.801736Z","shell.execute_reply":"2021-10-18T08:27:01.156213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path1 = '../input/covid19-ct-scans/ct_scans/coronacases_org_002.nii'\nmask_path1 = '../input/covid19-ct-scans/infection_mask/coronacases_002.nii'","metadata":{"execution":{"iopub.status.busy":"2021-10-18T08:27:01.158307Z","iopub.execute_input":"2021-10-18T08:27:01.158552Z","iopub.status.idle":"2021-10-18T08:27:01.163203Z","shell.execute_reply.started":"2021-10-18T08:27:01.15852Z","shell.execute_reply":"2021-10-18T08:27:01.162383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n# # # validation data\n# # %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n# # Initialize for stacking\n# val_data = torch.empty((1, 572, 572))\n# val_label = torch.empty((1, 572, 572))\n\n# # Function to form train data and train label\n# for i in val_data_numbers:\n#     file_path = data_path +arr[i]  # Path to validation data\n#     data = data_preprocess(file_path, HU_min, HU_max)  # Preprocess the data\n#     data = data.permute(2, 0,\n#                         1)  # change the dimension (H,W,C) ---> (C,H,W) , since ConstantPad2d works with this config\n#     P = padding_size(data)  # Obtain padding sizes\n#     data = torch.nn.ConstantPad2d(P, 0)(data)  # zero pad the slices\n#     val_data = torch.cat((val_data, data), 0)  # stack the data along dimension C\n\n#     file_path_mask = mask_path +arr1[i]\n#     label = mask_obtain(file_path_mask)  # Path to mask\n#     label = label.permute(2, 0, 1)  # change the dimension (H,W,C) ---> (C,H,W) , since ConstantPad2d works with this config\n#     label = torch.nn.ConstantPad2d(P, 0)(label)  # zero pad mask\n#     val_label = torch.cat((val_label, label), 0)  # stack the masks along dimension C\n\n# # remove the empty\n# val_data = val_data[1:val_data.size()[0], :, :]\n# val_label = val_label[1:val_label.size()[0], :, :]\n\n# # Determine which slices are all black\n# idx = []\n# for i in range(val_label.size()[0]):\n#     img_max = torch.max(val_label[i, :, :])\n#     if img_max == 1:\n#         idx.append(i)  # having white regions\n\n# # Choose data without completely black mask, i.e, having atleast some white segmented region\n# val_data_new = val_data[idx, :, :]\n# val_label_new = val_label[idx, :, :]\n\n# # (C,H,W) ---> (H,W,C) since Dataset class has this config (this part is not necessary if we change the config of\n# # Dataset class)\n# val_data_new = val_data_new.permute(1, 2, 0)\n# val_label_new = val_label_new.permute(1, 2, 0)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T08:27:01.164661Z","iopub.execute_input":"2021-10-18T08:27:01.165164Z","iopub.status.idle":"2021-10-18T08:27:01.172369Z","shell.execute_reply.started":"2021-10-18T08:27:01.165131Z","shell.execute_reply":"2021-10-18T08:27:01.171692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n# from torchvision.transforms.functional import sigmoid\n\ndef DCE(inputs, targets, smooth=1):\n    inputs = torch.sigmoid(inputs)\n\n    # flatten label and prediction tensors\n    inputs = inputs.view(-1)\n    targets = targets.view(-1)\n\n    intersection = (inputs * targets).sum()\n    dice = (2. * intersection+smooth) / (inputs.sum()+targets.sum()+smooth)\n    return 1-dice\n\n\ndef GDCE(inputs, targets, smooth=1):\n    inputs = F.sigmoid(inputs)\n\n    # flatten label and prediction tensors\n    inputs = inputs.view(-1)\n    targets = targets.view(-1)\n\n    # targets and corresponding predictions of class 1\n    idx1 = (targets == 0)\n    T1 = targets[idx1]\n    P1 = inputs[idx1]\n\n    # targets and corresponding predictions of class 2\n    idx2 = (targets == 1)\n    T2 = targets[idx2]\n    P2 = inputs[idx2]\n\n    # Weights for each class\n    W1 = 1 / (len(T1) * len(T1))\n    W2 = 1 / (len(T2) * len(T2))\n\n    # Numerator and denominator of generalized dice loss\n    NR = W1 * (T1 * P1).sum()+W2 * (T2 * P2).sum()\n    DR = W1 * (T1+P1).sum()+W2 * (T2+P2).sum()\n\n    loss = 1-(((2 * NR) + smooth)/ (DR+smooth))\n\n    return loss\n\ndef  FocalTverskyLoss(inputs, targets, smooth=1, alpha=0.7, beta=0.3, gamma=(4/3)):\n    # comment out if your model contains a sigmoid or equivalent activation layer\n    inputs = torch.sigmoid(inputs)\n\n    # flatten label and prediction tensors\n    inputs = inputs.view(-1)\n    targets = targets.view(-1)\n\n    # True Positives, False Positives & False Negatives\n    TP = (inputs * targets).sum()\n    FP = ((1-targets) * inputs).sum()\n    FN = (targets * (1-inputs)).sum()\n\n    Tversky = (TP+smooth) / (TP+alpha * FP+beta * FN+smooth)\n    FocalTversky = (1-Tversky) ** gamma\n\n    return FocalTversky","metadata":{"execution":{"iopub.status.busy":"2021-10-18T08:27:01.175083Z","iopub.execute_input":"2021-10-18T08:27:01.175306Z","iopub.status.idle":"2021-10-18T08:27:01.190964Z","shell.execute_reply.started":"2021-10-18T08:27:01.175257Z","shell.execute_reply":"2021-10-18T08:27:01.190305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import Module\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Two sequential convolution section of Unet\ndef doubleconv(inp, out):\n    double_conv = nn.Sequential(\n        nn.Conv2d(inp, out, kernel_size=3),\n        nn.BatchNorm2d(out,track_running_stats=False),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(out, out, kernel_size=3),\n        nn.BatchNorm2d(out,track_running_stats=False),\n        nn.ReLU(inplace=True)\n    )\n    return double_conv\n\n\n# Crop the encoder feature to the size of corresponding decoder for concatenation\ndef crop_feat1(input_tensor, target_tensor):\n    out_size = target_tensor.size()[2]\n    inp_size = input_tensor.size()[2]\n    delta = (inp_size-out_size) // 2\n    if (inp_size-out_size) % 2 == 0:\n        result = input_tensor[:, :, delta:inp_size-delta, delta:inp_size-delta]\n    else:\n        result = input_tensor[:, :, delta:inp_size-delta-1, delta:inp_size-delta-1]\n\n    return result\n\n\n# Spatial Channel Attention Block\nclass sca(Module):\n    def __init__(self, inp):\n        super(sca, self).__init__()\n        self.c_attn_conv = nn.Sequential(nn.Conv2d(inp, inp // 16, 1, bias=False),\n                                         nn.ReLU(),\n                                         nn.Conv2d(inp // 16, inp, 1, bias=False)\n                                         )\n        self.c_sig = nn.Sigmoid()\n\n        self.avg_ch = nn.AdaptiveAvgPool2d(1)\n        self.max_ch = nn.AdaptiveMaxPool2d(1)\n        self.s_attn_conv = nn.Sequential(nn.Conv2d(2, 1, kernel_size=7, padding=7 // 2, bias=False),\n                                         nn.Sigmoid())\n\n    def forward(self, input_tensor):\n        # Channel Attention\n        avg_ch_pool = self.avg_ch(input_tensor)\n        max_ch_pool = self.max_ch(input_tensor)\n\n        out_1 = self.c_attn_conv(avg_ch_pool)\n        out_2 = self.c_attn_conv(max_ch_pool)\n\n        c_sum = out_1+out_2\n        ch_out = self.c_sig(c_sum)\n        input_tensor = input_tensor * ch_out\n        # Spatial Attention\n        avg_pool = torch.mean(input_tensor, dim=1, keepdim=True)\n        max_pool = torch.max(input_tensor, dim=1, keepdim=True)\n        x = torch.cat([avg_pool, max_pool.values], dim=1)\n\n        x = self.s_attn_conv(x)\n        x = torch.mul(input_tensor, x)\n\n        return x\n\n\n# Function to apply spatial channel attention block\ndef spatial_channel_attn(input_tensor):\n    input_tensor = input_tensor.type(torch.cuda.FloatTensor)\n    inp = input_tensor.size()[1]\n\n    sca_model = sca(inp).to(device)\n    x = sca_model(input_tensor)\n\n    return x\n\n\n# Atrous spatial pyramid pooling block\nclass aspp(Module):\n    def __init__(self, inp, out):\n        super(aspp, self).__init__()\n        self.aconv0 = nn.Sequential(nn.Conv2d(inp, out, kernel_size=3, stride=1, padding=4, dilation=4, bias=False),\n                                    nn.BatchNorm2d(out,track_running_stats=False),\n                                    nn.ReLU(inplace=True))\n        self.aconv1 = nn.Sequential(nn.Conv2d(inp, out, kernel_size=3, stride=1, padding=6, dilation=6, bias=False),\n                                    nn.BatchNorm2d(out,track_running_stats=False),\n                                    nn.ReLU(inplace=True))\n        self.aconv2 = nn.Sequential(nn.Conv2d(inp, out, kernel_size=3, stride=1, padding=12, dilation=12, bias=False),\n                                    nn.BatchNorm2d(out,track_running_stats=False),\n                                    nn.ReLU(inplace=True))\n        self.aconv3 = nn.Sequential(nn.Conv2d(inp, out, kernel_size=3, stride=1, padding=18, dilation=18, bias=False),\n                                    nn.BatchNorm2d(out,track_running_stats=False),\n                                    nn.ReLU(inplace=True))\n        self.aconv4 = nn.Sequential(nn.Conv2d(inp, out, kernel_size=3, stride=1, padding=24, dilation=24, bias=False),\n                                    nn.BatchNorm2d(out,track_running_stats=False),\n                                    nn.ReLU(inplace=True))\n        self.final_conv = nn.Sequential(\n            nn.Conv2d(out * 5, inp, kernel_size=1, stride=1, padding=0, dilation=1, bias=False),\n            nn.BatchNorm2d(inp,track_running_stats=False),\n            nn.ReLU(inplace=True))\n\n    def forward(self, input_tensor):\n        x0 = self.aconv0(input_tensor)\n        x1 = self.aconv1(input_tensor)\n        x2 = self.aconv2(input_tensor)\n        x3 = self.aconv3(input_tensor)\n        x4 = self.aconv4(input_tensor)\n        x = torch.cat((x0, x1, x2, x3, x4), dim=1)\n        aspp_out = self.final_conv(x)\n\n        return aspp_out\n\n\n# Function to carry out atrous spatial pyramid pooling (In our case input and output is made to be of same size)\ndef atrous_spatial_pyramid_pooling(input_tensor):\n    input_tensor = input_tensor.type(torch.cuda.FloatTensor)\n    inp = input_tensor.size()[1]\n    out = inp // 4\n\n    asppmodel = aspp(inp, out).to(device)\n    aspp_out = asppmodel(input_tensor)\n    res_aspp = aspp_out+input_tensor\n    return res_aspp\n\n\n# Unet Model\nclass Unet(Module):\n    def __init__(self):\n        super(Unet, self).__init__()\n\n        self.maxpool2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.DownConv1 = doubleconv(1, 64)\n        self.DownConv2 = doubleconv(64, 128)\n        self.DownConv3 = doubleconv(128, 256)\n        self.DownConv4 = doubleconv(256, 512)\n        self.DownConv5 = doubleconv(512, 1024)\n\n        self.UpTrans1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n        self.UpTrans2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.UpTrans3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.UpTrans4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n\n        self.UpConv1 = doubleconv(1024, 512)\n        self.UpConv2 = doubleconv(512, 256)\n        self.UpConv3 = doubleconv(256, 128)\n        self.UpConv4 = doubleconv(128, 64)\n\n        self.out1 = nn.Conv2d(64, 1, kernel_size=1)\n        self.out2 = nn.Conv2d(128, 1, kernel_size=1)\n        self.out3 = nn.Conv2d(256, 1, kernel_size=1)\n        self.out4 = nn.Conv2d(512, 1, kernel_size=1)\n\n    def forward(self, x):\n        # x=(batch size, channel, height, width)\n        # encoder\n\n        x1 = self.DownConv1(x)\n        x2 = self.maxpool2x2(x1)\n        x3 = self.DownConv2(x2)\n        x4 = self.maxpool2x2(x3)\n        x5 = self.DownConv3(x4)\n        x6 = self.maxpool2x2(x5)\n        x7 = self.DownConv4(x6)\n        x8 = self.maxpool2x2(x7)\n        x9 = self.DownConv5(x8)\n\n        # ASPP\n        x9_aspp = atrous_spatial_pyramid_pooling(x9)\n\n        # decoder\n        z1 = self.UpTrans1(spatial_channel_attn(x9_aspp))\n        y1 = crop_feat1(x7, z1)\n        x10 = self.UpConv1(torch.cat([y1, z1], 1))\n\n        z2 = self.UpTrans2(spatial_channel_attn(x10))\n        y2 = crop_feat1(x5, z2)\n        x11 = self.UpConv2(torch.cat([y2, z2], 1))\n\n        z3 = self.UpTrans3(spatial_channel_attn(x11))\n        y3 = crop_feat1(x3, z3)\n        x12 = self.UpConv3(torch.cat([y3, z3], 1))\n\n        z4 = self.UpTrans4(spatial_channel_attn(x12))\n        y4 = crop_feat1(x1, z4)\n        x13 = self.UpConv4(torch.cat([y4, z4], 1))\n\n        # ASPP\n        x13_aspp = atrous_spatial_pyramid_pooling(x13)\n        out1 = self.out1(x13_aspp)\n        out2 = self.out2(x12)\n        out3 = self.out3(x11)\n        out4 = self.out4(x10)\n        return out1, out2, out3, out4\n\n# %%%%%%% model  check %%%%%%%%%\n\n# if __name__ == \"__main__\":\n#     image = torch.rand((2, 1, 512, 512))\n#     model = Unet()\n#     model.to(device)\n#     image=image.to(device)\n#     y = model(image)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T08:27:01.193177Z","iopub.execute_input":"2021-10-18T08:27:01.193717Z","iopub.status.idle":"2021-10-18T08:27:01.283997Z","shell.execute_reply.started":"2021-10-18T08:27:01.193682Z","shell.execute_reply":"2021-10-18T08:27:01.28318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms.functional as F\n# from data_preparation import train_data_new, train_label_new, val_data_new, val_label_new\n# from model_all import Unet\nfrom torch.optim import Adam\nimport torchvision.transforms as T\nimport warnings\nfrom torch.cuda import amp\nfrom tqdm import tqdm\n# from loss import DCE\nimport matplotlib.pyplot as plt\n\n# Ignore warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Create GradScaler for mixed precision training\nscaler = amp.GradScaler()\n\n# Choose the device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Function to crop mask to the size of output of U-net\ndef crop_feat(inp):\n    inp_size = inp.size()[2]  # here it is 572 as decided during data preparation\n    delta = (inp_size-388) // 2\n    return inp[:, delta:inp_size-delta, delta:inp_size-delta]\n\n\n# Function to augment data\n# Function to augment data\ndef data_augmentation(image, mask):\n    # Horizontal flip\n    if torch.rand(1) > 0.5:\n        image = F.hflip(image)\n        mask = F.hflip(mask)\n    # Vertical flip\n    if torch.rand(1) > 0.5:\n        image = F.vflip(image)\n        mask = F.vflip(mask)\n    # Rotate 90 degree or -90 degree\n    if torch.rand(1) > 0.5:\n        if torch.rand(1) > 0.5:\n            image = torch.rot90(image, 1, [0, 1])\n            mask = torch.rot90(mask, 1, [0, 1])\n        else:\n            image = torch.rot90(image, -1, [0, 1])\n            mask = torch.rot90(mask, -1, [0, 1])\n    return image, mask\n\n\n# Dataset class\nclass CovidSegData(Dataset):\n    def __init__(self, data, labels, transform=None):\n        self.data = data\n        self.transform = transform\n        self.labels = labels\n\n    def __len__(self):\n        return self.labels.size()[2]  # Data in (H,W,C) format obtained from data preparation\n\n    def __getitem__(self, item):\n        # Select each slice\n        slices = self.data[:, :, item]\n        masks = self.labels[:, :, item]\n        if self.transform is not None:  # if transform is True carry out data augmentation\n            slices, masks = data_augmentation(slices, masks)\n        return slices, masks\n\n\n# Loss function\ndef loss_function(preds1, preds2, preds3, preds4, GT):\n    # resize groundtruth according to the predicted label dimensions\n    GT1a = GT  # Ground truth already cropped to the output dimension of Unet\n    resize_mask2 = T.Resize(size=(preds2.size()[2], preds2.size()[3]))\n    GT2 = resize_mask2(GT)\n    resize_mask3 = T.Resize(size=(preds3.size()[2], preds3.size()[3]))\n    GT3 = resize_mask3(GT)\n    resize_mask4 = T.Resize(size=(preds4.size()[2], preds4.size()[3]))\n    GT4 = resize_mask4(GT)\n\n\n    D1f = FocalTverskyLoss(preds1, GT1a, gamma=1)\n    D2f = FocalTverskyLoss(preds2, GT2)\n    D3f = FocalTverskyLoss(preds3, GT3)\n    D4f = FocalTverskyLoss(preds4, GT4)\n    # weighted loss -- deep supervision\n    FTL_final = (0.5 * D1f)+(0.2 * D2f)+(0.2 * D3f)+(0.1 * D4f)\n\n    loss_final = FTL_final\n\n    return loss_final\n\n\n# Load data and dataloaders for training and validation\ntrain_dataset = CovidSegData(train_data_new, train_label_new, transform=True)\ntrain_loader = DataLoader(train_dataset, batch_size=2,num_workers=2, shuffle=True, pin_memory=True)\n# val_dataset = CovidSegData(val_data_new, val_label_new, transform=False)\n\n# val_loader = DataLoader(val_dataset, batch_size=2, num_workers=2, shuffle=False, pin_memory=True)\n\n# Initialize network\nmodel = Unet()#torch.load('/media/hp/DATA/CT scan work/Covid19 CTseg (Seg ICASSP)/model_all_rad_100epochs.pth')\nmodel.to(device)  # Move the model to GPU\n# Optimizer and Scheduler\noptimizer = Adam(model.parameters(), lr=2e-4, weight_decay=1e-6)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\nnum_epochs = 50  # number of epochs\nfor epoch in range(num_epochs):\n    loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=True)  # for progress bar display\n    for batch_idx, (images, GT1) in loop:\n        model.train()  # model in training mode\n        dataset_size = 0\n        running_loss = 0.0\n        # Crop mask to the size of output of Unet,i.e, 388 in this case\n        GT = crop_feat(GT1)\n        # Get data to cuda\n        images = images.to(device)\n        GT = GT.to(device)\n\n        batch_size = images.size()[0]  # Size of each batch\n        optimizer.zero_grad()  # zeroing gradients\n        images = torch.unsqueeze(images, 1)  # get correct input dimensions\n        images = images.type(torch.cuda.FloatTensor)  # Convert input to Float tensor\n        with amp.autocast():  # forward part with autocasting -- mixed precision training (MPT)\n            preds1, preds2, preds3, preds4 = model(images)  # predictions\n            loss = loss_function(preds1, preds2, preds3, preds4, GT)  # loss\n        scaler.scale(loss).backward()  # scales loss and create scaled gradients for MPT\n        # unscale the gradients of the optimizer assigned params, skips optimizer.step if Nan or Inf present\n        scaler.step(optimizer)\n        scaler.update()  # update scale for next iteration\n        scheduler.step()  # update learning scheduler\n\n        # Epoch loss calculation\n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        epoch_loss = running_loss / dataset_size\n        \n       \n        # Update progress bar\n        loop.set_description(f\"Epoch : [{epoch}/{num_epochs}]\")\n        loop.set_postfix(loss=loss.item(), epoch_loss=epoch_loss)\n    # Visualization ( the output of unet need to be passed through sigmoid and do thresholding at 0.5 to make it binary\n    # Final Output of Unet\n    print(epoch_loss)\n    z = torch.sigmoid(preds1) > 0.5\n    z1 = z[0].detach().cpu()\n    z2 = z1.squeeze()\n    # Intermediate output 1\n    t1 = torch.sigmoid(preds2) > 0.5\n    t2 = t1[0].detach().cpu().squeeze()\n    # Intermediate output  2\n    a1 = torch.sigmoid(preds3) > 0.5\n    a2 = a1[0].detach().cpu().squeeze()\n    # Intermediate output 3\n    b1 = torch.sigmoid(preds4) > 0.5\n    b2 = b1[0].detach().cpu().squeeze()\n    # Input slice\n    y1 = images[0].detach().cpu()\n    y = crop_feat(y1).squeeze()\n    # Mask -Ground Truth\n    p1 = GT[0].detach().cpu().squeeze()\n\n    # FIgure with all outputs, mask and input slice\n    f, axarr = plt.subplots(1, 6)\n    axarr[0].imshow(y, cmap='gray')\n    axarr[1].imshow(p1, cmap='gray')\n    axarr[2].imshow(z2, cmap='gray')\n    axarr[3].imshow(t2, cmap='gray')\n    axarr[4].imshow(a2, cmap='gray')\n    axarr[5].imshow(b2, cmap='gray')\n    plt.show()\n    #","metadata":{"execution":{"iopub.status.busy":"2021-10-18T08:27:01.285403Z","iopub.execute_input":"2021-10-18T08:27:01.28563Z","iopub.status.idle":"2021-10-18T09:01:36.204996Z","shell.execute_reply.started":"2021-10-18T08:27:01.285601Z","shell.execute_reply":"2021-10-18T09:01:36.204093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(),'./weights5.pth')","metadata":{"execution":{"iopub.status.busy":"2021-10-18T09:01:53.842235Z","iopub.execute_input":"2021-10-18T09:01:53.842824Z","iopub.status.idle":"2021-10-18T09:01:54.108144Z","shell.execute_reply.started":"2021-10-18T09:01:53.842788Z","shell.execute_reply":"2021-10-18T09:01:54.107384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\n\nclass conv2DBatchNormRelu(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size,stride,padding,\n                 bias=True,dilation=1,is_batchnorm=True):\n        super(conv2DBatchNormRelu,self).__init__()\n        if is_batchnorm:\n            self.cbr_unit=nn.Sequential(\n                nn.Conv2d(in_channels,out_channels,kernel_size=kernel_size,stride=stride,padding=padding,\n                          bias=bias,dilation=dilation),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            self.cbr_unit=nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,\n                          bias=bias, dilation=dilation),\n                nn.ReLU(inplace=True)\n            )\n\n    def forward(self,inputs):\n        outputs=self.cbr_unit(inputs)\n        return outputs\n\nclass segnetDown2(nn.Module):\n    def __init__(self,in_channels,out_channels):\n        super(segnetDown2,self).__init__()\n        self.conv1=conv2DBatchNormRelu(in_channels,out_channels,kernel_size=3,stride=1,padding=1)\n        self.conv2=conv2DBatchNormRelu(out_channels,out_channels,kernel_size=3,stride=1,padding=1)\n        self.maxpool_with_argmax=nn.MaxPool2d(kernel_size=2,stride=2,return_indices=True)\n\n    def forward(self,inputs):\n        outputs=self.conv1(inputs)\n        outputs=self.conv2(outputs)\n        unpooled_shape=outputs.size()\n        outputs,indices=self.maxpool_with_argmax(outputs)\n        return outputs,indices,unpooled_shape\n\nclass segnetDown3(nn.Module):\n    def __init__(self,in_channels,out_channels):\n        super(segnetDown3,self).__init__()\n        self.conv1=conv2DBatchNormRelu(in_channels,out_channels,kernel_size=3,stride=1,padding=1)\n        self.conv2=conv2DBatchNormRelu(out_channels,out_channels,kernel_size=3,stride=1,padding=1)\n        self.conv3=conv2DBatchNormRelu(out_channels,out_channels,kernel_size=3,stride=1,padding=1)\n        self.maxpool_with_argmax=nn.MaxPool2d(kernel_size=2,stride=2,return_indices=True)\n\n    def forward(self,inputs):\n        outputs=self.conv1(inputs)\n        outputs=self.conv2(outputs)\n        outputs=self.conv3(outputs)\n        unpooled_shape=outputs.size()\n        outputs,indices=self.maxpool_with_argmax(outputs)\n        return outputs,indices,unpooled_shape\n\n\nclass segnetUp2(nn.Module):\n    def __init__(self,in_channels,out_channels):\n        super(segnetUp2,self).__init__()\n        self.unpool=nn.MaxUnpool2d(2,2)\n        self.conv1=conv2DBatchNormRelu(in_channels,out_channels,kernel_size=3,stride=1,padding=1)\n        self.conv2=conv2DBatchNormRelu(out_channels,out_channels,kernel_size=3,stride=1,padding=1)\n\n    def forward(self,inputs,indices,output_shape):\n        outputs=self.unpool(inputs,indices=indices,output_size=output_shape)\n        outputs=self.conv1(outputs)\n        outputs=self.conv2(outputs)\n        return outputs\n\nclass segnetUp3(nn.Module):\n    def __init__(self,in_channels,out_channels):\n        super(segnetUp3,self).__init__()\n        self.unpool=nn.MaxUnpool2d(2,2)\n        self.conv1=conv2DBatchNormRelu(in_channels,out_channels,kernel_size=3,stride=1,padding=1)\n        self.conv2=conv2DBatchNormRelu(out_channels,out_channels,kernel_size=3,stride=1,padding=1)\n        self.conv3=conv2DBatchNormRelu(out_channels,out_channels,kernel_size=3,stride=1,padding=1)\n\n    def forward(self,inputs,indices,output_shape):\n        outputs=self.unpool(inputs,indices=indices,output_size=output_shape)\n        outputs=self.conv1(outputs)\n        outputs=self.conv2(outputs)\n        outputs=self.conv3(outputs)\n        return outputs\n\nclass segnet(nn.Module):\n    def __init__(self,in_channels=1,num_classes=1):\n        super(segnet,self).__init__()\n        self.down1=segnetDown2(in_channels=1,out_channels=64)\n        self.down2=segnetDown2(64,128)\n        self.down3=segnetDown3(128,256)\n        self.down4=segnetDown3(256,512)\n        self.down5=segnetDown3(512,1024)\n\n        self.up5=segnetUp3(1024,512)\n        self.up4=segnetUp3(512,256)\n        self.up3=segnetUp3(256,128)\n        self.up2=segnetUp2(128,64)\n        self.up1=segnetUp2(64,64)\n        \n        self.finconv=conv2DBatchNormRelu(64,num_classes,3,1,1)\n        self.out1 = conv2DBatchNormRelu(64, 1, 3,1,1)\n        self.out2 = conv2DBatchNormRelu(128, 1, 3,1,1)\n        self.out3 = conv2DBatchNormRelu(256, 1,3,1,1)\n        self.out4 = conv2DBatchNormRelu(512, 1, 3,1,1)\n\n    def forward(self,inputs):\n        down1,indices_1,unpool_shape1=self.down1(inputs)\n        down2,indices_2,unpool_shape2=self.down2(down1)\n        down3,indices_3,unpool_shape3=self.down3(down2)\n        down4,indices_4,unpool_shape4=self.down4(down3)\n        down5,indices_5,unpool_shape5=self.down5(down4)\n\n        up5=self.up5(down5,indices=indices_5,output_shape=unpool_shape5)\n        up4=self.up4(up5,indices=indices_4,output_shape=unpool_shape4)\n        up3=self.up3(up4,indices=indices_3,output_shape=unpool_shape3)\n        up2=self.up2(up3,indices=indices_2,output_shape=unpool_shape2)\n        up1=self.up1(up2,indices=indices_1,output_shape=unpool_shape1)\n        outputs=self.finconv(up1)\n        \n        out1 = self.out1(up2)\n        out2 = self.out2(up3)\n        out3 = self.out3(up4)\n        out4 = self.out4(up5)\n        return outputs,out1,out2,out3,out4\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms.functional as F\n# from data_preparation import train_data_new, train_label_new, val_data_new, val_label_new\n# from model_all import Unet\nfrom torch.optim import Adam\nimport torchvision.transforms as T\nimport warnings\nfrom torch.cuda import amp\nfrom tqdm import tqdm\n# from loss import DCE\nimport matplotlib.pyplot as plt\n\n# Ignore warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Create GradScaler for mixed precision training\nscaler = amp.GradScaler()\n\n# Choose the device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Function to crop mask to the size of output of U-net\ndef crop_feat(inp):\n    inp_size = inp.size()[2]  # here it is 572 as decided during data preparation\n    delta = (inp_size-388) // 2\n    return inp[:, delta:inp_size-delta, delta:inp_size-delta]\n\ndef data_augmentation(image, mask):\n    if torch.rand(1) > 0.5:\n        image = F.hflip(image)\n        mask = F.hflip(mask)\n    if torch.rand(1) > 0.5:\n        image = F.vflip(image)\n        mask = F.vflip(mask)\n    if torch.rand(1) > 0.5:\n        if torch.rand(1) > 0.5:\n            image = torch.rot90(image, 1, [0, 1])\n            mask = torch.rot90(mask, 1, [0, 1])\n        else:\n            image = torch.rot90(image, -1, [0, 1])\n            mask = torch.rot90(mask, -1, [0, 1])\n    return image, mask\n\nclass CovidSegData(Dataset):\n    def __init__(self, data, labels, transform=None):\n        self.data = data\n        self.transform = transform\n        self.labels = labels\n    def __len__(self):\n        return self.labels.size()[2]  # Data in (H,W,C) format obtained from data preparation\n    def __getitem__(self, item):\n        # Select each slice\n        slices = self.data[:, :, item]\n        masks = self.labels[:, :, item]\n        if self.transform is not None:  # if transform is True carry out data augmentation\n            slices, masks = data_augmentation(slices, masks)\n        return slices, masks\n\ndef loss_function(preds1, preds2, preds3, preds4, GT):\n    # resize groundtruth according to the predicted label dimensions\n    GT1a = GT  # Ground truth already cropped to the output dimension of Unet\n    resize_mask2 = T.Resize(size=(preds2.size()[2], preds2.size()[3]))\n    GT2 = resize_mask2(GT)\n    resize_mask3 = T.Resize(size=(preds3.size()[2], preds3.size()[3]))\n    GT3 = resize_mask3(GT)\n    resize_mask4 = T.Resize(size=(preds4.size()[2], preds4.size()[3]))\n    GT4 = resize_mask4(GT)\n\n\n    D1f = FocalTverskyLoss(preds1, GT1a, gamma=1)\n    D2f = FocalTverskyLoss(preds2, GT2)\n    D3f = FocalTverskyLoss(preds3, GT3)\n    D4f = FocalTverskyLoss(preds4, GT4)\n    # weighted loss -- deep supervision\n    FTL_final = (0.5 * D1f)+(0.2 * D2f)+(0.2 * D3f)+(0.1 * D4f)\n\n    loss_final = FTL_final\n\n    return loss_final\n\n# Load data and dataloaders for training and validation\ntrain_dataset = CovidSegData(train_data_new, train_label_new, transform=True)\ntrain_loader = DataLoader(train_dataset, batch_size=2,num_workers=2, shuffle=True, pin_memory=True)\n\n# val_dataset = CovidSegData(val_data_new, val_label_new, transform=False)\n# val_loader = DataLoader(val_dataset, batch_size=2, num_workers=2, shuffle=False, pin_memory=True)\n\n# Initialize network\nmodel = segnet() #torch.load('/media/hp/DATA/CT scan work/Covid19 CTseg (Seg ICASSP)/model_all_rad_100epochs.pth')\n# model1= torch.load('kaggle/input/k-fold-weights/weights2.pth')\nmodel.to(device)  # Move the model to GPU\n\n# Optimizer and Scheduler\noptimizer = Adam(model.parameters(), lr=2e-4, weight_decay=1e-6)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\nnum_epochs = 25  # number of epochs\nfor epoch in range(num_epochs):\n    loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=True)  # for progress bar display\n    for batch_idx, (images, GT1) in loop:\n        model.train()  # model in training mode\n        dataset_size = 0\n        running_loss = 0.0\n        # Crop mask to the size of output of Unet,i.e, 388 in this case\n#         print(\"before\", GT1.size())\n        GT = GT1\n#         print(\"After\", GT.size())\n        # Get data to cuda\n        images = images.to(device)\n        GT = GT.to(device)\n\n        batch_size = images.size()[0]  # Size of each batch\n        optimizer.zero_grad()  # zeroing gradients\n        images = torch.unsqueeze(images, 1)  # get correct input dimensions\n        images = images.type(torch.cuda.FloatTensor)  # Convert input to Float tensor\n        with amp.autocast():  # forward part with autocasting -- mixed precision training (MPT)\n            preds1, preds2, preds3, preds4 = model(images)  # predictions\n            loss = loss_function(preds1, preds2, preds3, preds4, GT)  # loss\n        scaler.scale(loss).backward()  # scales loss and create scaled gradients for MPT\n        # unscale the gradients of the optimizer assigned params, skips optimizer.step if Nan or Inf present\n        scaler.step(optimizer)\n        scaler.update()  # update scale for next iteration\n        scheduler.step()  # update learning scheduler\n\n        # Epoch loss calculation\n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        epoch_loss = running_loss / dataset_size\n        # Update progress bar\n        loop.set_description(f\"Epoch : [{epoch}/{num_epochs}]\")\n        loop.set_postfix(loss=loss.item(), epoch_loss=epoch_loss)\n    # Visualization ( the output of unet need to be passed through sigmoid and do thresholding at 0.5 to make it binary\n    # Final Output of Unet\n    z = torch.sigmoid(preds1) > 0.5\n    z1 = z[0].detach().cpu()\n    z2 = z1.squeeze()\n    # Input slice\n    y1 = images[0].detach().cpu()\n    y = crop_feat(y1).squeeze()\n    # Mask -Ground Truth\n    p1 = GT[0].detach().cpu().squeeze()\n\n    # FIgure with all outputs, mask and input slice\n    f, axarr = plt.subplots(1, 3)\n    axarr[0].imshow(y, cmap='gray')\n    axarr[1].imshow(p1, cmap='gray')\n    axarr[2].imshow(z2, cmap='gray')\n    plt.show()\n    #","metadata":{},"execution_count":null,"outputs":[]}]}