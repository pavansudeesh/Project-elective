{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-25T09:26:37.563173Z","iopub.execute_input":"2021-10-25T09:26:37.564191Z","iopub.status.idle":"2021-10-25T09:26:37.590383Z","shell.execute_reply.started":"2021-10-25T09:26:37.564144Z","shell.execute_reply":"2021-10-25T09:26:37.589659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd","metadata":{"execution":{"iopub.status.busy":"2021-10-25T09:26:37.593181Z","iopub.execute_input":"2021-10-25T09:26:37.593409Z","iopub.status.idle":"2021-10-25T09:26:37.601153Z","shell.execute_reply.started":"2021-10-25T09:26:37.593385Z","shell.execute_reply":"2021-10-25T09:26:37.600398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import shutil\n# source = '../input/unet-model/'\n# destination = '/root/'\n# shutil.move(source, destination) ","metadata":{"execution":{"iopub.status.busy":"2021-10-25T09:26:37.602816Z","iopub.execute_input":"2021-10-25T09:26:37.603338Z","iopub.status.idle":"2021-10-25T09:26:37.607049Z","shell.execute_reply.started":"2021-10-25T09:26:37.603305Z","shell.execute_reply":"2021-10-25T09:26:37.606122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# import sys\n# # insert at 1, 0 is the script path (or '' in REPL)\n# sys.path.insert(1, '../input/filetoimport')\n\n# import model_all\n# # os.chdir(\"../working/\")","metadata":{"execution":{"iopub.status.busy":"2021-10-25T09:26:37.609417Z","iopub.execute_input":"2021-10-25T09:26:37.610095Z","iopub.status.idle":"2021-10-25T09:26:37.614349Z","shell.execute_reply.started":"2021-10-25T09:26:37.610056Z","shell.execute_reply":"2021-10-25T09:26:37.613548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# import sys\n# # insert at 1, 0 is the script path (or '' in REPL)\n# sys.path.insert(1, '../input/unet-model')\n# # import modelall\n# import modelall as model_all\n# # os.chdir(\"../working/\")","metadata":{"execution":{"iopub.status.busy":"2021-10-25T09:26:37.615389Z","iopub.execute_input":"2021-10-25T09:26:37.616154Z","iopub.status.idle":"2021-10-25T09:26:37.622556Z","shell.execute_reply.started":"2021-10-25T09:26:37.616118Z","shell.execute_reply":"2021-10-25T09:26:37.621778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# os.chdir(\"../input/unet-model/\")\n# import modellall.py","metadata":{"execution":{"iopub.status.busy":"2021-10-25T09:26:37.623453Z","iopub.execute_input":"2021-10-25T09:26:37.624017Z","iopub.status.idle":"2021-10-25T09:26:37.630579Z","shell.execute_reply.started":"2021-10-25T09:26:37.623992Z","shell.execute_reply":"2021-10-25T09:26:37.629846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nibabel as nib\nimport os\nimport torch\nimport torchvision.transforms as T\n\n# Data original source downloaded from : https://zenodo.org/record/3757476#.Xp0FhB9fgUE\n# Same data in Kaggle  : https://www.kaggle.com/andrewmvd/covid19-ct-scans\n\n# Data and mask path\ndata_path = \"/kaggle/input/covid19-ct-scans/ct_scans/coronacases_org_001.nii\"\nmask_path = \"/kaggle/input/covid19-ct-scans/infection_mask/coronacases_001.nii\"\n\n# Lung window in HU units\nHU_min = -1000\nHU_max = 400\n\n# names of files in data folder\n# arr = os.listdir(data_path)\n\n# select train and val subjects\ntrain_data_numbers = range(0, 0)  # data from coronacases.org\nval_data_numbers = range(0, 1)  # data from radiopedia.org\n\n# Resizing function for image and mask according to the model input dimension\n# resize_image = T.Resize(size=(572, 572))\n\n\n# function to preprocess data\ndef data_preprocess(path, hu_min, hu_max):\n    volume_data = nib.load(path)  # load data\n    volume_data_numpy = volume_data.get_fdata()  # get data as numpy\n    volume_data_tensor = torch.tensor(volume_data_numpy)  # convert to torch tensor\n    volume_data_tensor_clamped = torch.clamp(volume_data_tensor, min=hu_min, max=hu_max)  # apply HU lung window\n    volume_data_tensor_clamped_normalized = (volume_data_tensor_clamped-hu_min) / (hu_max-hu_min)  # normalize to [0,1]\n    return volume_data_tensor_clamped_normalized\n\n\n# function to obtain maask\ndef mask_obtain(fpath):\n    mask = nib.load(fpath)  # load mask\n    mask_numpy = mask.get_fdata()  # get mask as numpy\n    mask_tensor = torch.tensor(mask_numpy)  # convert to torch tensor\n    return mask_tensor\n\n\n# add zero padding if size of image less than required input size\ndef padding_size(slices):\n    if (572-slices.size()[1]) % 2 == 0:  # See if the difference between required size and data size is even or odd\n        # if difference is even, pad same number to either side\n        pad1 = (572-slices.size()[1]) // 2\n        pad2 = (572-slices.size()[1]) // 2\n    else:\n        # if difference is even, pad one side one value more than other\n        pad1 = (572-slices.size()[1]) // 2\n        pad2 = ((572-slices.size()[1]) // 2)+1\n\n    if (572-slices.size()[2]) % 2 == 0:  # See if the difference between required size and data size is even or odd\n        # if difference is even, pad same number to either side\n        pad3 = (572-slices.size()[2]) // 2\n        pad4 = (572-slices.size()[2]) // 2\n    else:\n        # if difference is even, pad one side one value more than other\n        pad3 = (572-slices.size()[2]) // 2\n        pad4 = ((572-slices.size()[2]) // 2)+1\n\n    return [pad4, pad3, pad2, pad1]  # return the number of zero padding in each side of the slice","metadata":{"execution":{"iopub.status.busy":"2021-10-25T09:26:37.63333Z","iopub.execute_input":"2021-10-25T09:26:37.633572Z","iopub.status.idle":"2021-10-25T09:26:37.647522Z","shell.execute_reply.started":"2021-10-25T09:26:37.633549Z","shell.execute_reply":"2021-10-25T09:26:37.646686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test = data_preprocess(data_path,-1000,400)\n# test.size()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T09:26:37.815568Z","iopub.execute_input":"2021-10-25T09:26:37.815888Z","iopub.status.idle":"2021-10-25T09:26:37.819425Z","shell.execute_reply.started":"2021-10-25T09:26:37.815862Z","shell.execute_reply":"2021-10-25T09:26:37.818535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # validation data\n# Initialize for stacking\nval_data = torch.empty((1, 572, 572))\nval_label = torch.empty((1, 572, 572))\n\n# Function to form train data and train label\nfor i in val_data_numbers:\n    file_path = data_path  # Path to validation data\n    data = data_preprocess(file_path, HU_min, HU_max)  # Preprocess the data\n    data = data.permute(2, 0, 1)  # change the dimension (H,W,C) ---> (C,H,W) , since ConstantPad2d works with this config\n    P = padding_size(data)  # Obtain padding sizes\n    data = torch.nn.ConstantPad2d(P, 0)(data)  # zero pad the slices\n    val_data = torch.cat((val_data, data), 0)  # stack the data along dimension C\n\n    file_path_mask = mask_path\n    label = mask_obtain(file_path_mask)  # Path to mask\n    label = label.permute(2, 0, 1)  # change the dimension (H,W,C) ---> (C,H,W) , since ConstantPad2d works with this config\n    label = torch.nn.ConstantPad2d(P, 0)(label)  # zero pad mask\n    val_label = torch.cat((val_label, label), 0)  # stack the masks along dimension C\n\n# remove the empty\nval_data = val_data[1:val_data.size()[0], :, :]\nval_label = val_label[1:val_label.size()[0], :, :]\n\n# Determine which slices are all black\nidx = []\nfor i in range(val_label.size()[0]):\n    img_max = torch.max(val_label[i, :, :])\n    if img_max == 1:\n        idx.append(i)  # having white regions\n\n# Choose data without completely black mask, i.e, having atleast some white segmented region\nval_data_new = val_data[idx, :, :]\nval_label_new = val_label[idx, :, :]\n\n# (C,H,W) ---> (H,W,C) since Dataset class has this config (this part is not necessary if we change the config of\n# Dataset class)\nval_data_new = val_data_new.permute(1, 2, 0)\nval_label_new = val_label_new.permute(1, 2, 0)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T09:26:37.82163Z","iopub.execute_input":"2021-10-25T09:26:37.822313Z","iopub.status.idle":"2021-10-25T09:26:46.171224Z","shell.execute_reply.started":"2021-10-25T09:26:37.822276Z","shell.execute_reply":"2021-10-25T09:26:46.17048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_label_new.size()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T09:26:46.172625Z","iopub.execute_input":"2021-10-25T09:26:46.172893Z","iopub.status.idle":"2021-10-25T09:26:46.178198Z","shell.execute_reply.started":"2021-10-25T09:26:46.172858Z","shell.execute_reply":"2021-10-25T09:26:46.177503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n\n\ndef DCE(inputs, targets, smooth=1):\n    inputs = torch.sigmoid(inputs)\n\n    # flatten label and prediction tensors\n    inputs = inputs.view(-1)\n#     print(inputs.size())\n#     print(targets.size())\n    targets = targets.view(-1)\n\n    intersection = (inputs * targets).sum()\n    dice = (2. * intersection+smooth) / (inputs.sum()+targets.sum()+smooth)\n    return 1-dice\n\n\ndef GDCE(inputs, targets, smooth=1):\n    inputs = F.sigmoid(inputs)\n\n    # flatten label and prediction tensors\n    inputs = inputs.view(-1)\n    targets = targets.view(-1)\n\n    # targets and corresponding predictions of class 1\n    idx1 = (targets == 0)\n    T1 = targets[idx1]\n    P1 = inputs[idx1]\n\n    # targets and corresponding predictions of class 2\n    idx2 = (targets == 1)\n    T2 = targets[idx2]\n    P2 = inputs[idx2]\n\n    # Weights for each class\n    W1 = 1 / (len(T1) * len(T1))\n    W2 = 1 / (len(T2) * len(T2))\n\n    # Numerator and denominator of generalized dice loss\n    NR = W1 * (T1 * P1).sum()+W2 * (T2 * P2).sum()\n    DR = W1 * (T1+P1).sum()+W2 * (T2+P2).sum()\n\n    loss = 1-(((2 * NR) + smooth)/ (DR+smooth))\n\n    return loss","metadata":{"execution":{"iopub.status.busy":"2021-10-25T09:26:46.179655Z","iopub.execute_input":"2021-10-25T09:26:46.180183Z","iopub.status.idle":"2021-10-25T09:26:46.191642Z","shell.execute_reply.started":"2021-10-25T09:26:46.180147Z","shell.execute_reply":"2021-10-25T09:26:46.190917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import Module\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Two sequential convolution section of Unet\ndef doubleconv(inp, out):\n    double_conv = nn.Sequential(\n        nn.Conv2d(inp, out, kernel_size=3),\n        nn.BatchNorm2d(out,track_running_stats=False),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(out, out, kernel_size=3),\n        nn.BatchNorm2d(out,track_running_stats=False),\n        nn.ReLU(inplace=True)\n    )\n    return double_conv\n\n\n# Crop the encoder feature to the size of corresponding decoder for concatenation\ndef crop_feat1(input_tensor, target_tensor):\n    out_size = target_tensor.size()[2]\n    inp_size = input_tensor.size()[2]\n    delta = (inp_size-out_size) // 2\n    if (inp_size-out_size) % 2 == 0:\n        result = input_tensor[:, :, delta:inp_size-delta, delta:inp_size-delta]\n    else:\n        result = input_tensor[:, :, delta:inp_size-delta-1, delta:inp_size-delta-1]\n\n    return result\n\n\n# Spatial Channel Attention Block\nclass sca(Module):\n    def __init__(self, inp):\n        super(sca, self).__init__()\n        self.c_attn_conv = nn.Sequential(nn.Conv2d(inp, inp // 16, 1, bias=False),\n                                         nn.ReLU(),\n                                         nn.Conv2d(inp // 16, inp, 1, bias=False)\n                                         )\n        self.c_sig = nn.Sigmoid()\n\n        self.avg_ch = nn.AdaptiveAvgPool2d(1)\n        self.max_ch = nn.AdaptiveMaxPool2d(1)\n        self.s_attn_conv = nn.Sequential(nn.Conv2d(2, 1, kernel_size=7, padding=7 // 2, bias=False),\n                                         nn.Sigmoid())\n\n    def forward(self, input_tensor):\n        # Channel Attention\n        avg_ch_pool = self.avg_ch(input_tensor)\n        max_ch_pool = self.max_ch(input_tensor)\n\n        out_1 = self.c_attn_conv(avg_ch_pool)\n        out_2 = self.c_attn_conv(max_ch_pool)\n\n        c_sum = out_1+out_2\n        ch_out = self.c_sig(c_sum)\n        input_tensor = input_tensor * ch_out\n        # Spatial Attention\n        avg_pool = torch.mean(input_tensor, dim=1, keepdim=True)\n        max_pool = torch.max(input_tensor, dim=1, keepdim=True)\n        x = torch.cat([avg_pool, max_pool.values], dim=1)\n\n        x = self.s_attn_conv(x)\n        x = torch.mul(input_tensor, x)\n\n        return x\n\n\n# Function to apply spatial channel attention block\ndef spatial_channel_attn(input_tensor):\n    input_tensor = input_tensor.type(torch.cuda.FloatTensor)\n    inp = input_tensor.size()[1]\n\n    sca_model = sca(inp).to(device)\n    x = sca_model(input_tensor)\n\n    return x\n\n\n# Atrous spatial pyramid pooling block\nclass aspp(Module):\n    def __init__(self, inp, out):\n        super(aspp, self).__init__()\n        self.aconv0 = nn.Sequential(nn.Conv2d(inp, out, kernel_size=3, stride=1, padding=4, dilation=4, bias=False),\n                                    nn.BatchNorm2d(out,track_running_stats=False),\n                                    nn.ReLU(inplace=True))\n        self.aconv1 = nn.Sequential(nn.Conv2d(inp, out, kernel_size=3, stride=1, padding=6, dilation=6, bias=False),\n                                    nn.BatchNorm2d(out,track_running_stats=False),\n                                    nn.ReLU(inplace=True))\n        self.aconv2 = nn.Sequential(nn.Conv2d(inp, out, kernel_size=3, stride=1, padding=12, dilation=12, bias=False),\n                                    nn.BatchNorm2d(out,track_running_stats=False),\n                                    nn.ReLU(inplace=True))\n        self.aconv3 = nn.Sequential(nn.Conv2d(inp, out, kernel_size=3, stride=1, padding=18, dilation=18, bias=False),\n                                    nn.BatchNorm2d(out,track_running_stats=False),\n                                    nn.ReLU(inplace=True))\n        self.aconv4 = nn.Sequential(nn.Conv2d(inp, out, kernel_size=3, stride=1, padding=24, dilation=24, bias=False),\n                                    nn.BatchNorm2d(out,track_running_stats=False),\n                                    nn.ReLU(inplace=True))\n        self.final_conv = nn.Sequential(\n            nn.Conv2d(out * 5, inp, kernel_size=1, stride=1, padding=0, dilation=1, bias=False),\n            nn.BatchNorm2d(inp),\n            nn.ReLU(inplace=True))\n\n    def forward(self, input_tensor):\n        x0 = self.aconv0(input_tensor)\n        x1 = self.aconv1(input_tensor)\n        x2 = self.aconv2(input_tensor)\n        x3 = self.aconv3(input_tensor)\n        x4 = self.aconv4(input_tensor)\n        x = torch.cat((x0, x1, x2, x3, x4), dim=1)\n        aspp_out = self.final_conv(x)\n\n        return aspp_out\n\n\n# Function to carry out atrous spatial pyramid pooling (In our case input and output is made to be of same size)\ndef atrous_spatial_pyramid_pooling(input_tensor):\n    input_tensor = input_tensor.type(torch.cuda.FloatTensor)\n    inp = input_tensor.size()[1]\n    out = inp // 4\n\n    asppmodel = aspp(inp, out).to(device)\n    aspp_out = asppmodel(input_tensor)\n    res_aspp = aspp_out+input_tensor\n    return res_aspp\n\n\n# Unet Model\nclass Unet(Module):\n    def __init__(self):\n        super(Unet, self).__init__()\n\n        self.maxpool2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.DownConv1 = doubleconv(1, 64)\n        self.DownConv2 = doubleconv(64, 128)\n        self.DownConv3 = doubleconv(128, 256)\n        self.DownConv4 = doubleconv(256, 512)\n        self.DownConv5 = doubleconv(512, 1024)\n\n        self.UpTrans1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n        self.UpTrans2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.UpTrans3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.UpTrans4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n\n        self.UpConv1 = doubleconv(1024, 512)\n        self.UpConv2 = doubleconv(512, 256)\n        self.UpConv3 = doubleconv(256, 128)\n        self.UpConv4 = doubleconv(128, 64)\n\n        self.out1 = nn.Conv2d(64, 1, kernel_size=1)\n        self.out2 = nn.Conv2d(128, 1, kernel_size=1)\n        self.out3 = nn.Conv2d(256, 1, kernel_size=1)\n        self.out4 = nn.Conv2d(512, 1, kernel_size=1)\n\n    def forward(self, x):\n        # x=(batch size, channel, height, width)\n        # encoder\n\n        x1 = self.DownConv1(x)\n        x2 = self.maxpool2x2(x1)\n        x3 = self.DownConv2(x2)\n        x4 = self.maxpool2x2(x3)\n        x5 = self.DownConv3(x4)\n        x6 = self.maxpool2x2(x5)\n        x7 = self.DownConv4(x6)\n        x8 = self.maxpool2x2(x7)\n        x9 = self.DownConv5(x8)\n\n        # ASPP\n        x9_aspp = atrous_spatial_pyramid_pooling(x9)\n\n        # decoder\n        z1 = self.UpTrans1(spatial_channel_attn(x9_aspp))\n        y1 = crop_feat1(x7, z1)\n        x10 = self.UpConv1(torch.cat([y1, z1], 1))\n\n        z2 = self.UpTrans2(spatial_channel_attn(x10))\n        y2 = crop_feat1(x5, z2)\n        x11 = self.UpConv2(torch.cat([y2, z2], 1))\n\n        z3 = self.UpTrans3(spatial_channel_attn(x11))\n        y3 = crop_feat1(x3, z3)\n        x12 = self.UpConv3(torch.cat([y3, z3], 1))\n\n        z4 = self.UpTrans4(spatial_channel_attn(x12))\n        y4 = crop_feat1(x1, z4)\n        x13 = self.UpConv4(torch.cat([y4, z4], 1))\n\n        # ASPP\n        x13_aspp = atrous_spatial_pyramid_pooling(x13)\n        out1 = self.out1(x13_aspp)\n        out2 = self.out2(x12)\n        out3 = self.out3(x11)\n        out4 = self.out4(x10)\n        return out1, out2, out3, out4\n\n# %%%%%%% model  check %%%%%%%%%\n\n# if __name__ == \"__main__\":\n#     image = torch.rand((2, 1, 572, 572))\n#     model = Unet()\n#     model.to(device)\n#     image=image.to(device)\n#     y = model(image)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T09:26:46.194389Z","iopub.execute_input":"2021-10-25T09:26:46.195085Z","iopub.status.idle":"2021-10-25T09:26:46.235074Z","shell.execute_reply.started":"2021-10-25T09:26:46.195013Z","shell.execute_reply":"2021-10-25T09:26:46.234418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms.functional as F\n# from data_preparation import train_data_new, train_label_new, val_data_new, val_label_new\nfrom torch.optim import Adam\nimport torchvision.transforms as T\nimport warnings\nfrom torch.cuda import amp\nfrom tqdm import tqdm\n# from loss import DCE\nimport matplotlib.pyplot as plt\n\n# Ignore warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Create GradScaler for mixed precision training\nscaler = amp.GradScaler()\n\n# Choose the device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Function to crop mask to the size of output of U-net\ndef crop_feat(inp):\n    inp_size = inp.size()[2]  # here it is 572 as decided during data preparation\n    delta = (inp_size-388) // 2\n    return inp[:, delta:inp_size-delta, delta:inp_size-delta]\n\n\n# Function to augment data\ndef data_augmentation(image, mask):\n    # Horizontal flip\n    if torch.rand(1) > 0.5:\n        image = F.hflip(image)\n        mask = F.hflip(mask)\n    # Vertical flip\n    if torch.rand(1) > 0.5:\n        image = F.vflip(image)\n        mask = F.vflip(mask)\n    # Rotate 90 degree or -90 degree\n    if torch.rand(1) > 0.5:\n        if torch.rand(1) > 0.5:\n            image = torch.rot90(image, 1, [0, 1])\n            mask = torch.rot90(mask, 1, [0, 1])\n        else:\n            image = torch.rot90(image, -1, [0, 1])\n            mask = torch.rot90(mask, -1, [0, 1])\n    return image, mask\n\n\n# Dataset class\nclass CovidSegData(Dataset):\n    def __init__(self, data, labels, transform=None):\n        self.data = data\n        self.transform = transform\n        self.labels = labels\n\n    def __len__(self):\n        return self.labels.size()[2]  # Data in (H,W,C) format obtained from data preparation\n\n    def __getitem__(self, item):\n        # Select each slice\n        slices = self.data[:, :, item]\n        masks = self.labels[:, :, item]\n        if self.transform is not None:  # if transform is True carry out data augmentation\n            slices, masks = data_augmentation(slices, masks)\n        return slices, masks\n\n\n# Loss function\ndef loss_function(preds1, preds2, preds3, preds4, GT):\n    # resize groundtruth according to the predicted label dimensions\n    GT1a = GT  # Ground truth already cropped to the output dimension of Unet\n    resize_mask2 = T.Resize(size=(preds2.size()[2], preds2.size()[3]))\n    GT2 = resize_mask2(GT)\n    resize_mask3 = T.Resize(size=(preds3.size()[2], preds3.size()[3]))\n    GT3 = resize_mask3(GT)\n    resize_mask4 = T.Resize(size=(preds4.size()[2], preds4.size()[3]))\n    GT4 = resize_mask4(GT)\n    # Dice score for each predictions\n    D1 = DCE(preds1, GT1a)\n    D2 = DCE(preds2, GT2)\n    D3 = DCE(preds3, GT3)\n    D4 = DCE(preds4, GT4)\n    # weighted loss -- deep supervision\n    loss_final = (0.5 * D1)+(0.2 * D2)+(0.1 * D3)+(0.1 * D4)\n    return loss_final","metadata":{"execution":{"iopub.status.busy":"2021-10-25T09:26:46.236695Z","iopub.execute_input":"2021-10-25T09:26:46.237005Z","iopub.status.idle":"2021-10-25T09:26:46.364367Z","shell.execute_reply.started":"2021-10-25T09:26:46.236971Z","shell.execute_reply":"2021-10-25T09:26:46.363605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_dataset = CovidSegData(val_data_new, val_label_new, transform=False)\n\nval_loader = DataLoader(val_dataset, batch_size=4,num_workers=2, shuffle=False, pin_memory=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-25T09:26:46.365436Z","iopub.execute_input":"2021-10-25T09:26:46.365953Z","iopub.status.idle":"2021-10-25T09:26:46.375986Z","shell.execute_reply.started":"2021-10-25T09:26:46.365916Z","shell.execute_reply":"2021-10-25T09:26:46.375307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize network\nmodel = Unet()\nlosses=0\ni =0\nmodel.load_state_dict(torch.load('/kaggle/input/50epoches/all_weights/weights1 (1).pth'), strict=False)\n# model = (torch.load('/kaggle/input/k-fold-weights/weights1.pth'))\nmodel.to(device)  # Move the model to GPU\n# Optimizer and Scheduler\noptimizer = Adam(model.parameters(), lr=2e-4, weight_decay=1e-6)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\nnum_epochs = 1  # number of epochs\nfor epoch in range(num_epochs):\n    # Model evaluation - validation mode\n    model.eval()  # Set model in evaluation mode\n    with torch.no_grad():  # Disable gradient calculation\n        loop = tqdm(enumerate(val_loader), total=len(val_loader), leave=True)  # For progress bar\n        for batch_idx, (images, GT1) in loop:\n            dataset_size = 0\n            running_loss = 0.0\n            # Crop mask to the size of output of Unet,i.e, 388 in this case\n            GT = crop_feat(GT1)\n            # Get data to cuda\n            images = images.to(device)\n            GT = GT.to(device)\n            batch_size = images.size()[0]  # Size of each batch\n            images = torch.unsqueeze(images, 1)  # get correct input dimensions\n            images = images.type(torch.cuda.FloatTensor)  # Convert input to Float tensor\n            preds1, preds2, preds3, preds4 = model(images)  # predictions\n            loss = loss_function(preds1, preds2, preds3, preds4, GT)  # loss\n           \n            \n            # Epoch loss calculation\n            running_loss += (loss.item() * batch_size)\n            dataset_size += batch_size\n            epoch_loss = running_loss / dataset_size\n            losses= losses+ epoch_loss\n#             print(epoch_loss)\n            i = i+1\n#             # Update progress bar\n            loop.set_postfix(loss=loss.item(), epoch_loss=epoch_loss)\n            \n            # Visualize image, mask and predicted mask\n            # Pass the prediction through sigmoid to get probabilities and threshold at 0.5 to obtain binary image\n#             print(torch.max(torch.sigmoid(preds1)),torch.min(torch.sigmoid(preds1)) )\n            \n            z = preds1\n            z[z<0.1] = 0\n#             torch.sigmoid(preds1)*(10**9) > 0.5\n            z1 = z[0].detach().cpu()\n            z2 = z1.squeeze()\n            # Input Slice\n            y1 = images[0].detach().cpu()\n            y = crop_feat(y1).squeeze()\n            # Mask or Ground truth\n            p1 = GT[0].detach().cpu().squeeze()\n\n#             Figure with input image, groundtrith and predicted mask\n#             f, axarr = plt.subplots(1, 3)\n#             axarr[0].imshow(y, cmap='gray')\n#             axarr[1].imshow(p1, cmap='gray')\n#             axarr[2].imshow(z2, cmap='gray')\n#             plt.show()\nprint(losses/i)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T09:26:46.37782Z","iopub.execute_input":"2021-10-25T09:26:46.378425Z","iopub.status.idle":"2021-10-25T09:27:01.517474Z","shell.execute_reply.started":"2021-10-25T09:26:46.37839Z","shell.execute_reply":"2021-10-25T09:27:01.514792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"losses=0\ni =0\n# Initialize network\nmodel = Unet()\nmodel.load_state_dict(torch.load('/kaggle/input/50epoches/all_weights/weights2 (1).pth'), strict=False)\n# model = (torch.load('/kaggle/input/k-fold-weights/weights1.pth'))\nmodel.to(device)  # Move the model to GPU\n# Optimizer and Scheduler\noptimizer = Adam(model.parameters(), lr=2e-4, weight_decay=1e-6)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\nnum_epochs = 1  # number of epochs\nfor epoch in range(num_epochs):\n    # Model evaluation - validation mode\n    model.eval()  # Set model in evaluation mode\n    with torch.no_grad():  # Disable gradient calculation\n        loop = tqdm(enumerate(val_loader), total=len(val_loader), leave=True)  # For progress bar\n        for batch_idx, (images, GT1) in loop:\n            dataset_size = 0\n            running_loss = 0.0\n            # Crop mask to the size of output of Unet,i.e, 388 in this case\n            GT = crop_feat(GT1)\n            # Get data to cuda\n            images = images.to(device)\n            GT = GT.to(device)\n\n            batch_size = images.size()[0]  # Size of each batch\n\n            images = torch.unsqueeze(images, 1)  # get correct input dimensions\n\n            images = images.type(torch.cuda.FloatTensor)  # Convert input to Float tensor\n\n            preds1, preds2, preds3, preds4 = model(images)  # predictions\n            loss = loss_function(preds1, preds2, preds3, preds4, GT)  # loss\n            # Epoch loss calculation\n            running_loss += (loss.item() * batch_size)\n            dataset_size += batch_size\n            epoch_loss = running_loss / dataset_size\n            losses= losses+ epoch_loss\n            i = i+1\n            # Update progress bar\n            loop.set_postfix(loss=loss.item(), epoch_loss=epoch_loss)\n            # Visualize image, mask and predicted mask\n            # Pass the prediction through sigmoid to get probabilities and threshold at 0.5 to obtain binary image\n#             print(torch.max(torch.sigmoid(preds1)),torch.min(torch.sigmoid(preds1)) )\n            \n#             z = torch.sigmoid(preds1)*(10**9) > 0.5\n            z = preds1\n            z[z<0.1] = 0\n            z1 = z[0].detach().cpu()\n            z2 = z1.squeeze()\n            # Input Slice\n            y1 = images[0].detach().cpu()\n            y = crop_feat(y1).squeeze()\n            # Mask or Ground truth\n            p1 = GT[0].detach().cpu().squeeze()\n\n            # Figure with input image, groundtrith and predicted mask\n#             f, axarr = plt.subplots(1, 3)\n#             axarr[0].imshow(y, cmap='gray')\n#             axarr[1].imshow(p1, cmap='gray')\n#             axarr[2].imshow(z2, cmap='gray')\n#             plt.show()\nprint(losses/i)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T09:27:01.519193Z","iopub.execute_input":"2021-10-25T09:27:01.519581Z","iopub.status.idle":"2021-10-25T09:27:17.340596Z","shell.execute_reply.started":"2021-10-25T09:27:01.51954Z","shell.execute_reply":"2021-10-25T09:27:17.339775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize network\nlosses=0\ni =0\nmodel = Unet()\nmodel.load_state_dict(torch.load('/kaggle/input/50epoches/all_weights/weights3 (1).pth'), strict=False)\n# model = (torch.load('/kaggle/input/k-fold-weights/weights1.pth'))\nmodel.to(device)  # Move the model to GPU\n# Optimizer and Scheduler\noptimizer = Adam(model.parameters(), lr=2e-4, weight_decay=1e-6)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\nnum_epochs = 1  # number of epochs\nfor epoch in range(num_epochs):\n    # Model evaluation - validation mode\n    model.eval()  # Set model in evaluation mode\n    with torch.no_grad():  # Disable gradient calculation\n        loop = tqdm(enumerate(val_loader), total=len(val_loader), leave=True)  # For progress bar\n        for batch_idx, (images, GT1) in loop:\n            dataset_size = 0\n            running_loss = 0.0\n            # Crop mask to the size of output of Unet,i.e, 388 in this case\n            GT = crop_feat(GT1)\n            # Get data to cuda\n            images = images.to(device)\n            GT = GT.to(device)\n\n            batch_size = images.size()[0]  # Size of each batch\n\n            images = torch.unsqueeze(images, 1)  # get correct input dimensions\n\n            images = images.type(torch.cuda.FloatTensor)  # Convert input to Float tensor\n\n            preds1, preds2, preds3, preds4 = model(images)  # predictions\n            loss = loss_function(preds1, preds2, preds3, preds4, GT)  # loss\n            # Epoch loss calculation\n            running_loss += (loss.item() * batch_size)\n            dataset_size += batch_size\n            epoch_loss = running_loss / dataset_size\n            losses= losses+ epoch_loss\n            i = i+1\n            # Update progress bar\n            loop.set_postfix(loss=loss.item(), epoch_loss=epoch_loss)\n            # Visualize image, mask and predicted mask\n            # Pass the prediction through sigmoid to get probabilities and threshold at 0.5 to obtain binary image\n#             print(torch.max(torch.sigmoid(preds1)),torch.min(torch.sigmoid(preds1)) )\n            \n#             z = torch.sigmoid(preds1)*(10**9) > 0.5\n            z = preds1\n            z[z<0.1] = 0\n            z1 = z[0].detach().cpu()\n            z2 = z1.squeeze()\n            # Input Slice\n            y1 = images[0].detach().cpu()\n            y = crop_feat(y1).squeeze()\n            # Mask or Ground truth\n            p1 = GT[0].detach().cpu().squeeze()\n\n            # Figure with input image, groundtrith and predicted mask\n#             f, axarr = plt.subplots(1, 3)\n#             axarr[0].imshow(y, cmap='gray')\n#             axarr[1].imshow(p1, cmap='gray')\n#             axarr[2].imshow(z2, cmap='gray')\n#             plt.show()\nprint(losses/i)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T09:27:17.342312Z","iopub.execute_input":"2021-10-25T09:27:17.342589Z","iopub.status.idle":"2021-10-25T09:27:32.612766Z","shell.execute_reply.started":"2021-10-25T09:27:17.34255Z","shell.execute_reply":"2021-10-25T09:27:32.61197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"losses=0\ni =0\n# Initialize network\nmodel = Unet()\nmodel.load_state_dict(torch.load('/kaggle/input/50epoches/all_weights/weights4 (1).pth'), strict=False)\n# model = (torch.load('/kaggle/input/k-fold-weights/weights1.pth'))\nmodel.to(device)  # Move the model to GPU\n# Optimizer and Scheduler\noptimizer = Adam(model.parameters(), lr=2e-4, weight_decay=1e-6)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\nnum_epochs = 1  # number of epochs\nfor epoch in range(num_epochs):\n    # Model evaluation - validation mode\n    model.eval()  # Set model in evaluation mode\n    with torch.no_grad():  # Disable gradient calculation\n        loop = tqdm(enumerate(val_loader), total=len(val_loader), leave=True)  # For progress bar\n        for batch_idx, (images, GT1) in loop:\n            dataset_size = 0\n            running_loss = 0.0\n            # Crop mask to the size of output of Unet,i.e, 388 in this case\n            GT = crop_feat(GT1)\n            # Get data to cuda\n            images = images.to(device)\n            GT = GT.to(device)\n\n            batch_size = images.size()[0]  # Size of each batch\n\n            images = torch.unsqueeze(images, 1)  # get correct input dimensions\n\n            images = images.type(torch.cuda.FloatTensor)  # Convert input to Float tensor\n\n            preds1, preds2, preds3, preds4 = model(images)  # predictions\n            loss = loss_function(preds1, preds2, preds3, preds4, GT)  # loss\n            # Epoch loss calculation\n            running_loss += (loss.item() * batch_size)\n            dataset_size += batch_size\n            epoch_loss = running_loss / dataset_size\n            losses= losses+ epoch_loss\n            i = i+1\n            # Update progress bar\n            loop.set_postfix(loss=loss.item(), epoch_loss=epoch_loss)\n            # Visualize image, mask and predicted mask\n            # Pass the prediction through sigmoid to get probabilities and threshold at 0.5 to obtain binary image\n#             print(torch.max(torch.sigmoid(preds1)),torch.min(torch.sigmoid(preds1)) )\n            \n#             z = torch.sigmoid(preds1)*(10**9) > 0.5\n            z = preds1\n            z[z<0.1] = 0\n            z1 = z[0].detach().cpu()\n            z2 = z1.squeeze()\n            # Input Slice\n            y1 = images[0].detach().cpu()\n            y = crop_feat(y1).squeeze()\n            # Mask or Ground truth\n            p1 = GT[0].detach().cpu().squeeze()\n\n#             Figure with input image, groundtrith and predicted mask\n#             f, axarr = plt.subplots(1, 3)\n#             axarr[0].imshow(y, cmap='gray')\n#             axarr[1].imshow(p1, cmap='gray')\n#             axarr[2].imshow(z2, cmap='gray')\n#             plt.show()\nprint(losses/i)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T09:27:32.614575Z","iopub.execute_input":"2021-10-25T09:27:32.614855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# losses=0\n# i =0\n# # Initialize network\n# model = Unet()\n# model.load_state_dict(torch.load('/kaggle/input/50epoches/all_weights/weights5 (1).pth'), strict=False)\n# # model = (torch.load('/kaggle/input/k-fold-weights/weights1.pth'))\n# model.to(device)  # Move the model to GPU\n# # Optimizer and Scheduler\n# optimizer = Adam(model.parameters(), lr=2e-4, weight_decay=1e-6)\n# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n# num_epochs = 1  # number of epochs\n# for epoch in range(num_epochs):\n#     # Model evaluation - validation mode\n#     model.eval()  # Set model in evaluation mode\n#     with torch.no_grad():  # Disable gradient calculation\n#         loop = tqdm(enumerate(val_loader), total=len(val_loader), leave=True)  # For progress bar\n#         for batch_idx, (images, GT1) in loop:\n#             dataset_size = 0\n#             running_loss = 0.0\n#             # Crop mask to the size of output of Unet,i.e, 388 in this case\n#             GT = crop_feat(GT1)\n#             # Get data to cuda\n#             images = images.to(device)\n#             GT = GT.to(device)\n\n#             batch_size = images.size()[0]  # Size of each batch\n\n#             images = torch.unsqueeze(images, 1)  # get correct input dimensions\n\n#             images = images.type(torch.cuda.FloatTensor)  # Convert input to Float tensor\n\n#             preds1, preds2, preds3, preds4 = model(images)  # predictions\n#             loss = loss_function(preds1, preds2, preds3, preds4, GT)  # loss\n#             # Epoch loss calculation\n#             running_loss += (loss.item() * batch_size)\n#             dataset_size += batch_size\n#             epoch_loss = running_loss / dataset_size\n#             losses= losses+ epoch_loss\n#             i = i+1\n#             # Update progress bar\n#             loop.set_postfix(loss=loss.item(), epoch_loss=epoch_loss)\n#             # Visualize image, mask and predicted mask\n#             # Pass the prediction through sigmoid to get probabilities and threshold at 0.5 to obtain binary image\n# #             print(torch.max(torch.sigmoid(preds1)),torch.min(torch.sigmoid(preds1)) )\n            \n# #             z = torch.sigmoid(preds1)*(10**9) > 0.5\n#             z = preds1\n#             z[z<0.1] = 0\n#             z1 = z[0].detach().cpu()\n#             z2 = z1.squeeze()\n#             # Input Slice\n#             y1 = images[0].detach().cpu()\n#             y = crop_feat(y1).squeeze()\n#             # Mask or Ground truth\n#             p1 = GT[0].detach().cpu().squeeze()\n\n#             # Figure with input image, groundtrith and predicted mask\n# #             f, axarr = plt.subplots(1, 3)\n# #             axarr[0].imshow(y, cmap='gray')\n# #             axarr[1].imshow(p1, cmap='gray')\n# #             axarr[2].imshow(z2, cmap='gray')\n# #             plt.show()\n# print(losses/i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}